---
title: "Cómo se hace"
date: 2023-11-20T18:00:14+01:00
excludeFromTopNav: true
showDate: false
aliases:
    - /page/teoria-openai/
---

Esta es una teoría que escribí el 20 de noviembre de 2023 a las 18:10 sobre los motivos que hayan podido llevar a la salida de Sam Altman de OpenAI. La dejo aquí como registro. Me parece bastante poco probable, pero la única que se me ocurre que da una explicación coherente a lo ocurrido el pasado fin de semana:

A lo que se refirió Sam Altman en la última conferencia cuando dijo que lo que nos iban a presentar el año que viene iba a ser mucho más sorprendente no es una AGI, sino una versión mucho mejor de lo que actualmente se llaman agentes. Hay varios ejemplos por ahí desde hace tiempo: AutoGPT  babyagi por ejemplo. Son sistemas a los que les pides un objetivo general y ejecutan en bucle peticiones a los modelos de lenguaje para llevar a cabo tareas más complejas. El problema es que muchas veces se quedan atascados o pierden el contexto de lo que deben hacer.

Mi teoría es que OpenAI ha conseguido que los agentes funcionen bien. Eso significa ciertos puestos de trabajo, todo lo que es trabajo de oficina clásico de entrada, pero muchísimos más, quedan instantáneamente obsoletos. Sam Altman quiere sacar esta tecnología para que se use cuanto antes, y en la Junta hay gente que lo ha visto estas semanas y son más conservadores. Por eso han tomado la decisión, quizás también por presiones políticas, de desacelerar mucho el avance y la publicación. Y de ahí la elección de poner un CEO que en numerosas ocasiones haya dicho que hay que ir muy poco a poco en el tema del desarrollo de la Inteligencia Artificial.

Es la única cosa que se me ocurre para que tenga una explicación que no es una completa locura. Porque viendo lo que ya hay, si han dado una vueltecita de tuerca más a los GPTs para que puedan funcionar en modo bucle, no es algo impensable que hayan conseguido que funcione bien el tema de los agentes. No necesitan ni haber entrenado un modelo nuevo para eso.

Pero claro, ya dependes de cómo tú definas una AGI. Porque si una AGI = "puede hacer el trabajo básico de oficina"... Pues por poder, lo puedes llamar AGI. Aunque no tenga conciencia ni nada de eso.

También esto explicaría que Ilyas estuviera de acuerdo con la Junta en desacelerar este avance. Pero que después de ver el trato que han recibido sus amigos haya reculado. Porque quizás estaba de acuerdo en el fondo, pero no le han gustado nada las formas.

Encaja esta teoría con que Sam Altman y Greg Brockman quisieran montar súper rápido una nueva startup diciendo que van a hacer cosas increíbles. Sabiendo que OpenAI iba a retrasarlo, a ellos les podía dar tiempo a reconstruir lo que necesitan. Porque tienen el conocimiento para hacerlo. Si tú no tienes un objetivo muy claro, altamente novedoso, y sabes cómo conseguirlo, creo que sería poco prudente dar un mensaje como ese.